\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[margin=0.8in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}

% Title information
\title{\textbf{Advanced Natural Language Processing} \\

       \large Assignment 1: Twitter hate speech detection using Naive Bayes and
Logistic Regression}
\author{Lucia Welther, Matrikelnummer: 835106}
\date{\today}

\begin{document}

\maketitle

% ============================================================
\section{Smoothing Parameter Analysis}
% ============================================================

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{test_smooth.png}
    \caption{Accuracy and F1-score as functions of smoothing parameter \( k \)}
    \label{fig:smoothing}
\end{figure}

\textbf{Observations from Figure~\ref{fig:smoothing}:}
\begin{itemize}
\item Very small smoothing values (e.g.\ $k=0.001$) lead to overfitting: the model depends too much on exact training frequencies and handles unseen words poorly.
\item Moderate smoothing (e.g.\ $k=0.5$) gives the best performance by balancing observed counts with reasonable estimates for unseen words.
\item Very large smoothing values (e.g.\ $k=10$) over-smooth the probabilities, making word distinctions weak and reducing accuracy.
\end{itemize}

% ============================================================
\section{Feature Engineering}
% ============================================================

\subsection{Feature Set 1: Stopword Removal}

Removing stopwords reduces noise by eliminating very frequent and
non-informative words. Stopwords 
occur in both classes and do not help distinguish offensive from 
non-offensive tweets. After removing them, the model focuses on 
more meaningful content words, improving accuracy and F1-score.

\subsection{Feature Set 2: Bigrams}

Bigrams capture word order and context, allowing the model to distinguish 
phrases like ``not good'' from ``good''. Offensive language often contains specific 
multi-word expressions that bigrams can identify. By combining unigrams and 
bigrams, the model learns both individual words and contextual word pairs.

\begin{table}[H]
\centering
\caption{Performance comparison of feature engineering approaches}
\label{tab:features}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Feature Set} & \textbf{Accuracy} & \textbf{F1-Score} \\ 
\midrule
Baseline & 0.8572 &  0.7159 \\
Stopword Removal (Feature 1) & 0.8597 & 0.7209 \\
Bigrams (Feature 2) & 0.8548 & 0.6672 \\
\bottomrule
\end{tabular}
\end{table}

As seen in Table~\ref{tab:features}, Feature 1 (stopword removal) slightly improves 
both metrics (accuracy: 0.8597, F1: 0.7209), demonstrating that removing 
non-discriminative words helps focus on meaningful content. However, 
Feature 2 (bigrams) decreases performance (accuracy: 0.8548, F1: 0.6672), 
likely due to vocabulary expansion causing data sparsity issues. The increased 
feature space introduces many rare bigrams with unreliable probability estimates, 
leading to overfitting. This suggests that for this dataset size, vocabulary reduction 
is more effective than feature expansion.

% ============================================================
\section{Model Evaluation Results}
% ============================================================

The Naive Bayes classifier with optimal smoothing parameter \( k = 0.5 \) and the 
Logistic Regression classifier with learning rate \( \eta = 0.01 \), 10 epochs, 
and L2 regularization parameter \( C = 0.1 \) achieved:
\begin{table}[H]
\centering
\caption{Performance comparison of Naive Bayes vs. Logistic Regression}
\label{tab:models}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{F1-Score} \\ 
\midrule
Naive Bayes & 0.8572 & 0.7159 \\
Logistic Regression & 0.8148 & 0.4506 \\
\bottomrule
\end{tabular}
\end{table}

As shown in Table~\ref{tab:models}, Naive Bayes significantly outperforms 
Logistic Regression (F1: 0.7159 vs. 0.4506). Despite Logistic Regression's 
flexibility, Naive Bayes excels due to appropriate independence assumptions, 
resistance to overfitting on small data, and sufficient training compared to 
Logistic Regression's limited 10 epochs. This demonstrates that simpler models 
can be more effective with limited training data.

% ============================================================
% BONUS SECTION (if applicable)
% ============================================================
\section{Bonus: Discussion Questions}

\begin{itemize}

    \item \textbf{Effect of Smoothing Parameter:}
    The smoothing parameter $k$ prevents zero probabilities for unseen words by adding pseudocounts; very small $k$ may cause overfitting, while very large $k$ makes word probabilities nearly uniform and harms model performance.

    \item \textbf{Word Order Shuffling:}
    Shuffling word order does not affect Naive Bayes or bag-of-words logistic regression because both treat documents as unordered collections of words.

    \item \textbf{Purpose of Softmax Function:}
    Softmax converts raw class scores into normalized probabilities that sum to 1, enabling meaningful comparison between classes.

    \item \textbf{Decision Threshold Tuning:}
    The decision threshold can be adjusted after training based on validation or test predictions, so no retraining is required.

\end{itemize}


\subsection{GitHub Repository}

GitHub Repository: \url{https://github.com/luzecode/aNLP-Assignment1-Hate-Speech-Detection.git}

\textbf{Last commit hash:} \texttt{First Commit: NLP Assignment1}





\end{document}